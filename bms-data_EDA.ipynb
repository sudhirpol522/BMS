{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom multiprocessing import cpu_count\nfrom tqdm.notebook import tqdm\n\nimport cv2\nimport imageio\nimport joblib\nimport os\nimport pickle\n\nfrom PIL import Image, ImageFilter\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T15:58:54.997399Z","iopub.execute_input":"2021-06-16T15:58:54.997855Z","iopub.status.idle":"2021-06-16T15:59:01.844228Z","shell.execute_reply.started":"2021-06-16T15:58:54.997764Z","shell.execute_reply":"2021-06-16T15:59:01.843080Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Mean ratio of first 10K images is 1.73, this image width/height result in a ratio of 1.75\nDEBUG = True\nIMG_HEIGHT = 256\nIMG_WIDTH = 448\nVAL_SIZE = int(100) if DEBUG else int(100e3) # 100K validation molecules\nCHUNK_SIZE = 40000 # to get ~100MB TFRecords\n\n\nMAX_INCHI_LEN = 200 # maximum InChI length to prevent to much padding","metadata":{"execution":{"iopub.status.busy":"2021-06-16T15:59:01.845570Z","iopub.execute_input":"2021-06-16T15:59:01.845862Z","iopub.status.idle":"2021-06-16T15:59:01.851370Z","shell.execute_reply.started":"2021-06-16T15:59:01.845834Z","shell.execute_reply":"2021-06-16T15:59:01.850387Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    train = pd.read_csv('/kaggle/input/bms-molecular-translation/train_labels.csv', dtype={ 'image_id': 'string', 'InChI': 'string' }).head(int(1e3))\nelse:\n    train = pd.read_csv('/kaggle/input/bms-molecular-translation/train_labels.csv', dtype={ 'image_id': 'string', 'InChI': 'string' })\n\n# Drop all InChI longer than MAX_INCHI_LEN - 2,  <start>InChI <end>, remove 'InChI=1S/' at start\ntrain['InChI_len'] = train['InChI'].apply(len).astype(np.uint16)\ntrain = train.loc[train['InChI_len'] <= MAX_INCHI_LEN - 2 + 9].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T15:59:01.853189Z","iopub.execute_input":"2021-06-16T15:59:01.853458Z","iopub.status.idle":"2021-06-16T15:59:11.860010Z","shell.execute_reply.started":"2021-06-16T15:59:01.853432Z","shell.execute_reply":"2021-06-16T15:59:11.858669Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(train.info())","metadata":{"execution":{"iopub.status.busy":"2021-06-16T15:59:11.862502Z","iopub.execute_input":"2021-06-16T15:59:11.863111Z","iopub.status.idle":"2021-06-16T15:59:11.884645Z","shell.execute_reply.started":"2021-06-16T15:59:11.863047Z","shell.execute_reply":"2021-06-16T15:59:11.883843Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 966 entries, 0 to 965\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   image_id   966 non-null    string\n 1   InChI      966 non-null    string\n 2   InChI_len  966 non-null    uint16\ndtypes: string(2), uint16(1)\nmemory usage: 17.1 KB\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"display(train.head(3))","metadata":{"execution":{"iopub.status.busy":"2021-06-16T15:59:11.885633Z","iopub.execute_input":"2021-06-16T15:59:11.886037Z","iopub.status.idle":"2021-06-16T15:59:11.912633Z","shell.execute_reply.started":"2021-06-16T15:59:11.886006Z","shell.execute_reply":"2021-06-16T15:59:11.911438Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"       image_id                                              InChI  InChI_len\n0  000011a64c74  InChI=1S/C13H20OS/c1-9(2)8-15-13-6-5-10(3)7-12...         81\n1  000019cc0cd2  InChI=1S/C21H30O4/c1-12(22)25-14-6-8-20(2)13(1...        155\n2  0000252b6d2b  InChI=1S/C24H23N5O4/c1-14-13-15(7-8-17(14)28-1...        158","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>InChI</th>\n      <th>InChI_len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000011a64c74</td>\n      <td>InChI=1S/C13H20OS/c1-9(2)8-15-13-6-5-10(3)7-12...</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000019cc0cd2</td>\n      <td>InChI=1S/C21H30O4/c1-12(22)25-14-6-8-20(2)13(1...</td>\n      <td>155</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0000252b6d2b</td>\n      <td>InChI=1S/C24H23N5O4/c1-14-13-15(7-8-17(14)28-1...</td>\n      <td>158</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"if DEBUG:\n    test = pd.read_csv('/kaggle/input/bms-molecular-translation/sample_submission.csv', usecols=['image_id'], dtype={ 'image_id': 'string' }).head(int(1e3))\nelse:\n    test = pd.read_csv('/kaggle/input/bms-molecular-translation/sample_submission.csv', usecols=['image_id'], dtype={ 'image_id': 'string' })","metadata":{"execution":{"iopub.status.busy":"2021-06-16T15:59:11.914364Z","iopub.execute_input":"2021-06-16T15:59:11.915173Z","iopub.status.idle":"2021-06-16T15:59:13.697227Z","shell.execute_reply.started":"2021-06-16T15:59:11.915123Z","shell.execute_reply":"2021-06-16T15:59:13.696061Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(test.info())","metadata":{"execution":{"iopub.status.busy":"2021-06-16T15:59:13.700243Z","iopub.execute_input":"2021-06-16T15:59:13.700742Z","iopub.status.idle":"2021-06-16T15:59:13.714511Z","shell.execute_reply.started":"2021-06-16T15:59:13.700694Z","shell.execute_reply":"2021-06-16T15:59:13.713439Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 1 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   image_id  1000 non-null   string\ndtypes: string(1)\nmemory usage: 7.9 KB\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"display(test.head(3))","metadata":{"execution":{"iopub.status.busy":"2021-06-16T15:59:13.717064Z","iopub.execute_input":"2021-06-16T15:59:13.717706Z","iopub.status.idle":"2021-06-16T15:59:13.733030Z","shell.execute_reply.started":"2021-06-16T15:59:13.717658Z","shell.execute_reply":"2021-06-16T15:59:13.731643Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"       image_id\n0  00000d2a601c\n1  00001f7fc849\n2  000037687605","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00000d2a601c</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00001f7fc849</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000037687605</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Now get the Total Number of unique characters present in our InChi string\ndef get_vocabulary():\n    tokens = ['<start>', '<end>', '<pad>']\n    vocabulary = set()\n    for s in tqdm(train['InChI'].values):\n        vocabulary.update(s)\n    return tokens + list(vocabulary)\n\nvocabulary = get_vocabulary()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T15:59:13.734819Z","iopub.execute_input":"2021-06-16T15:59:13.735260Z","iopub.status.idle":"2021-06-16T15:59:13.783367Z","shell.execute_reply.started":"2021-06-16T15:59:13.735215Z","shell.execute_reply":"2021-06-16T15:59:13.782641Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/966 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9ef816bda454cc6ae8d8c1a87755c15"}},"metadata":{}}]},{"cell_type":"code","source":"# convert Inchi to Integer and save as pickle file so that we can restore it whenever we want\n# Save vocabulary mappings\n# , character -> integer\nvocabulary_to_int = dict(zip(vocabulary, np.arange(len(vocabulary), dtype=np.int8)))\n\nwith open('vocabulary_to_int.pkl', 'wb') as handle:\n    pickle.dump(vocabulary_to_int, handle)\n\n#  integer -> character\nint_to_vocabulary = dict(zip(np.arange(len(vocabulary), dtype=np.int8), vocabulary))\nwith open('int_to_vocabulary.pkl', 'wb') as handle:\n    pickle.dump(int_to_vocabulary, handle)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T15:59:13.784436Z","iopub.execute_input":"2021-06-16T15:59:13.784836Z","iopub.status.idle":"2021-06-16T15:59:13.791435Z","shell.execute_reply.started":"2021-06-16T15:59:13.784806Z","shell.execute_reply":"2021-06-16T15:59:13.790547Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## The Standard InChI is a fully standardized InChI flavor which always maintains the same level of attention to structure details and the same conventions for drawing perception. Its hashed counterpart is the Standard InChIKey.\n\nThe Standard InChI is designated by the prefix:\n\nInChI=1S/\nthat is, letter ‘S’ immediately follows the version number; Standard InChI version numbers are always whole numbers.","metadata":{}},{"cell_type":"code","source":"# Remove the \"InChI=1S/\" part from the InChI strings\n# It is equal for all InChI's, thus redundant or of useless while prediction\ntrain['InChIClean'] = train['InChI'].apply(lambda InChI: '/'.join(InChI.split('=')[1].split('/')[1:]))","metadata":{"execution":{"iopub.status.busy":"2021-06-16T15:59:13.792730Z","iopub.execute_input":"2021-06-16T15:59:13.793251Z","iopub.status.idle":"2021-06-16T15:59:13.810767Z","shell.execute_reply.started":"2021-06-16T15:59:13.793204Z","shell.execute_reply":"2021-06-16T15:59:13.809819Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"arr=train['InChIClean'].apply(lambda x: len(x))\nprint(\"10th percentile of arr : \", \n       np.percentile(arr, 10))\nprint(\"25th percentile of arr : \",\n       np.percentile(arr, 25))\nprint(\"40th percentile of arr : \", \n       np.percentile(arr, 40))\nprint(\"50th percentile of arr : \", \n       np.percentile(arr, 50))\n\nprint(\"75th percentile of arr : \",\n       np.percentile(arr, 75))\nprint(\"90th percentile of arr : \",\n       np.percentile(arr, 90))\nprint(\"95th percentile of arr : \",\n       np.percentile(arr, 95))\nprint(\"98th percentile of arr : \",\n       np.percentile(arr, 98))\nprint(\"99th percentile of arr : \",\n       np.percentile(arr, 99))","metadata":{"execution":{"iopub.status.busy":"2021-06-16T16:08:37.639247Z","iopub.execute_input":"2021-06-16T16:08:37.639784Z","iopub.status.idle":"2021-06-16T16:08:37.655557Z","shell.execute_reply.started":"2021-06-16T16:08:37.639750Z","shell.execute_reply":"2021-06-16T16:08:37.654549Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"10th percentile of arr :  76.0\n25th percentile of arr :  92.0\n40th percentile of arr :  103.0\n50th percentile of arr :  113.0\n75th percentile of arr :  137.0\n90th percentile of arr :  160.0\n95th percentile of arr :  172.0\n98th percentile of arr :  183.69999999999993\n99th percentile of arr :  189.35000000000002\n","output_type":"stream"}]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme(style=\"whitegrid\")\nplt.hist(train['InChIClean'].apply(lambda x: len(x)),cumulative=True, density=True,label='CDF', alpha=0.8, color='k',histtype='step')","metadata":{"execution":{"iopub.status.busy":"2021-06-16T16:01:43.317678Z","iopub.execute_input":"2021-06-16T16:01:43.318032Z","iopub.status.idle":"2021-06-16T16:01:44.355806Z","shell.execute_reply.started":"2021-06-16T16:01:43.318002Z","shell.execute_reply":"2021-06-16T16:01:44.354850Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(array([0.01759834, 0.11076605, 0.25879917, 0.45134576, 0.59937888,\n        0.75362319, 0.86438923, 0.93581781, 0.97826087, 1.        ]),\n array([ 47. ,  62.1,  77.2,  92.3, 107.4, 122.5, 137.6, 152.7, 167.8,\n        182.9, 198. ]),\n [<matplotlib.patches.Polygon at 0x7f32e1fc9b50>])"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## As we can see from the percentile, we could have select the maximum length as 95th percentile value that is 172, but as a sake of competetion we are going to select the length as 200.","metadata":{}},{"cell_type":"code","source":"# convert the InChI strings to integer lists\n# start/end/pad tokens are used\ndef inchi_str2int(InChI):\n    res = []\n    res.append(vocabulary_to_int.get('<start>'))\n    for c in InChI:\n        res.append(vocabulary_to_int.get(c))\n    \n    res.append(vocabulary_to_int.get('<end>'))\n    while len(res) < MAX_INCHI_LEN: \n        res.append(vocabulary_to_int.get('<pad>'))\n        \n    return np.array(res, dtype=np.uint8)\n        \ntrain['InChI_int'] = train['InChIClean'].progress_apply(inchi_str2int)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:30:05.943108Z","iopub.execute_input":"2021-05-25T13:30:05.943549Z","iopub.status.idle":"2021-05-25T13:30:06.095648Z","shell.execute_reply.started":"2021-05-25T13:30:05.943511Z","shell.execute_reply":"2021-05-25T13:30:06.094459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now calculate Image width/Image height ratio for first 10k images on the basis of this we will decide the size of the image\nws = []\nhs = []\nfor image_id in tqdm(train.loc[:int(10e3), 'image_id'].values):\n    file_path =  f'/kaggle/input/bms-molecular-translation/train/{image_id[0]}/{image_id[1]}/{image_id[2]}/{image_id}.png'\n    h, w = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE).shape\n    ws.append(w)\n    hs.append(h)\n    \nws_mean = int(np.array(ws).mean())\nhs_mean = int(np.array(hs).mean())\nprint(f'mean width: {ws_mean}, mean height: {hs_mean}, mean ratio: {round(ws_mean / hs_mean, 2)}')","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:30:09.22214Z","iopub.execute_input":"2021-05-25T13:30:09.222615Z","iopub.status.idle":"2021-05-25T13:30:17.316498Z","shell.execute_reply.started":"2021-05-25T13:30:09.222576Z","shell.execute_reply":"2021-05-25T13:30:17.315448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now divide our Train into Validation and Train Dataset\nval = train.iloc[-VAL_SIZE:].reset_index(drop=True)\ntrain = train.iloc[:-VAL_SIZE].reset_index(drop=True)\nN_IMGS = len(train)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:30:17.31818Z","iopub.execute_input":"2021-05-25T13:30:17.318561Z","iopub.status.idle":"2021-05-25T13:30:17.3258Z","shell.execute_reply.started":"2021-05-25T13:30:17.318525Z","shell.execute_reply":"2021-05-25T13:30:17.324573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Total images in the training dataset is {N_IMGS}')\nprint(f'Total images in the Validation dataset is {len(val)}')","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:30:19.638198Z","iopub.execute_input":"2021-05-25T13:30:19.638609Z","iopub.status.idle":"2021-05-25T13:30:19.644156Z","shell.execute_reply.started":"2021-05-25T13:30:19.638573Z","shell.execute_reply":"2021-05-25T13:30:19.643071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.options.display.max_colwidth = 100\ndisplay(train.head(3))","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:30:22.466848Z","iopub.execute_input":"2021-05-25T13:30:22.467524Z","iopub.status.idle":"2021-05-25T13:30:22.490917Z","shell.execute_reply.started":"2021-05-25T13:30:22.467461Z","shell.execute_reply":"2021-05-25T13:30:22.489284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.info())","metadata":{"execution":{"iopub.status.busy":"2021-05-24T10:19:35.080268Z","iopub.execute_input":"2021-05-24T10:19:35.080562Z","iopub.status.idle":"2021-05-24T10:19:35.094184Z","shell.execute_reply.started":"2021-05-24T10:19:35.080537Z","shell.execute_reply":"2021-05-24T10:19:35.093409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(val.info())","metadata":{"execution":{"iopub.status.busy":"2021-05-24T10:19:35.09509Z","iopub.execute_input":"2021-05-24T10:19:35.095385Z","iopub.status.idle":"2021-05-24T10:19:35.149052Z","shell.execute_reply.started":"2021-05-24T10:19:35.095359Z","shell.execute_reply":"2021-05-24T10:19:35.148141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://learnopencv.com/otsu-thresholding-with-opencv/ -> Othsu's Thresholding Algorithm\ndef crop(img, debug=False):\n    if debug:\n        fig, ax = plt.subplots(1,2, figsize=(30,8))\n        ax[0].imshow(img)\n        ax[0].set_title(f'original image, shape: {img.shape}', size=16)\n    # In this we are going to use OTSu's Thresholding because if we are going to do it manually, during each tunnng result's will be diffrent.\n    \n        \n    _, thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) # it needs Grey image\n    #https://docs.opencv.org/master/d9/d8b/tutorial_py_contours_hierarchy.html->cv2.RETR_LIST\n    contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE) #it needs black and white image\n    x_min, y_min, x_max, y_max = np.inf, np.inf, 0, 0\n    # https://docs.opencv.org/3.1.0/dd/d49/tutorial_py_contour_features.html\n    for cnt in contours:\n        \n        x, y, w, h = cv2.boundingRect(cnt) #cv2.boundingRect takes coordinates(x,y) and gives the Approximate Box around that object\n        x_min = min(x_min, x)\n        y_min = min(y_min, y)\n        x_max = max(x_max, x + w)\n        y_max = max(y_max, y + h)\n\n    img_cropped = img[y_min:y_max, x_min:x_max]\n    \n    if debug:\n        ax[1].imshow(img_cropped)\n        ax[1].set_title(f'cropped image, shape: {img_cropped.shape}', size=16)\n        plt.show()\n    \n    return img_cropped","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:32:25.371414Z","iopub.execute_input":"2021-05-25T13:32:25.371887Z","iopub.status.idle":"2021-05-25T13:32:25.383418Z","shell.execute_reply.started":"2021-05-25T13:32:25.371845Z","shell.execute_reply":"2021-05-25T13:32:25.381993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## cv2.findContours Function","metadata":{}},{"cell_type":"markdown","source":"## \nThe third argument in cv.findContours function. What does it denote actually?\n\nAbove, we told that contours are the boundaries of a shape with same intensity. It stores the (x,y) coordinates of the boundary of a shape. But does it store all the coordinates ? That is specified by this contour approximation method.\n\nIf you pass cv.CHAIN_APPROX_NONE, all the boundary points are stored. But actually do we need all the points? For eg, you found the contour of a straight line. Do you need all the points on the line to represent that line? No, we need just two end points of that line. This is what cv.CHAIN_APPROX_SIMPLE does. It removes all redundant points and compresses the contour, thereby saving memory.","metadata":{}},{"cell_type":"code","source":"def pad_resize(img):\n   \n    img = cv2.resize(img,(IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_NEAREST) #Image resize\n\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:32:28.660325Z","iopub.execute_input":"2021-05-25T13:32:28.660748Z","iopub.status.idle":"2021-05-25T13:32:28.667039Z","shell.execute_reply.started":"2021-05-25T13:32:28.660714Z","shell.execute_reply":"2021-05-25T13:32:28.665647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_img(image_id, folder='train', debug=False):\n    # read image and invert colors to get black background and white molecule\n    file_path =  f'/kaggle/input/bms-molecular-translation/{folder}/{image_id[0]}/{image_id[1]}/{image_id[2]}/{image_id}.png'\n    img0 = 255 - cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n    \n    # rotate counter clockwise to get horizontal images\n    h, w = img0.shape\n    if h > w:\n        img0 = np.rot90(img0)\n    \n    img = crop(img0, debug=debug)\n  \n    img = pad_resize(img0)\n    \n    if debug:\n        print('In')\n        fig, ax = plt.subplots(1, 2, figsize=(20,10))\n        ax[0].imshow(img0)\n        ax[0].set_title('Original image', size=16)\n        ax[1].imshow(img)\n        ax[1].set_title('Fully processed image', size=16)\n    \n    # normalize to range 0-255 and encode as png\n    img = (img / img.max() * 255).astype(np.uint8)\n    img = cv2.imencode('.png', img)[1].tobytes()\n\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:32:28.888935Z","iopub.execute_input":"2021-05-25T13:32:28.889369Z","iopub.status.idle":"2021-05-25T13:32:28.899408Z","shell.execute_reply.started":"2021-05-25T13:32:28.889327Z","shell.execute_reply":"2021-05-25T13:32:28.898177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Temp=process_img('000011a64c74',debug=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:35:55.950185Z","iopub.execute_input":"2021-05-25T13:35:55.95071Z","iopub.status.idle":"2021-05-25T13:35:56.766873Z","shell.execute_reply.started":"2021-05-25T13:35:55.950665Z","shell.execute_reply":"2021-05-25T13:35:56.76552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is used to create Chunk of TfRecords of equal Length\ndef split_in_chunks(data):\n    return [data[i:i + CHUNK_SIZE] for i in range(0, len(data), CHUNK_SIZE)]\n\ntrain_data_chunks = {\n    'train': {\n        'image_id': split_in_chunks(train['image_id'].values),\n        'InChI': split_in_chunks(train['InChI_int'].values),\n    },\n    'val': {\n        'image_id': split_in_chunks(val['image_id'].values),\n        'InChI': split_in_chunks(val['InChI_int'].values),\n    }\n}\n\ntest_data_chunks = {\n    'test': {\n        'image_id': split_in_chunks(test['image_id'].values),\n    }\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# refer: https://www.kaggle.com/xhlulu/flowers-tpu-concise-efficientnet-b7\n\n# This is the cell where TFRecords are being generated\ndef make_tfrecords(data_chunks, folder='train'):\n    # Try to make output folder\n    try:\n        os.makedirs(f'./train')\n        os.makedirs(f'./val')\n        os.makedirs(f'./test')\n    except:\n        print(f'folders already created')\n\n    for k, v in data_chunks.items():\n        for chunk_idx, image_id_chunk in tqdm(enumerate(v['image_id']), total=len(v['image_id'])):\n            # process images in parallel\n            jobs = [joblib.delayed(process_img)(fp, folder) for fp in image_id_chunk]\n            bs = 10\n            processed_images_chunk = joblib.Parallel(\n                n_jobs=cpu_count(),\n                verbose=0,\n                require='sharedmem',\n                batch_size=bs,\n                backend='threading',\n            )(jobs)\n\n            # Create the TFRecords from the processed images\n            with tf.io.TFRecordWriter(f'./{k}/batch_{chunk_idx}.tfrecords') as file_writer:\n                if 'InChI' in v.keys(): # TRAIN/VAL, InChI included\n                    for image, InChI in zip(processed_images_chunk, v['InChI'][chunk_idx]):\n                        record_bytes = tf.train.Example(features=tf.train.Features(feature={\n                            'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),\n                            'InChI': tf.train.Feature(int64_list=tf.train.Int64List(value=InChI)),\n                        })).SerializeToString()\n                        file_writer.write(record_bytes)\n                else: # TEST, image_id included for submission file\n                    for image, image_id in zip(processed_images_chunk, image_id_chunk):\n                        record_bytes = tf.train.Example(features=tf.train.Features(feature={\n                            'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),\n                            'image_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[str.encode(image_id)])),\n                        })).SerializeToString()\n                        file_writer.write(record_bytes)\n\nmake_tfrecords(train_data_chunks)\nmake_tfrecords(test_data_chunks, 'test')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## below cells are used for upload our dataset of TFRecords to KaggleDataset","metadata":{}},{"cell_type":"code","source":"! mkdir -p /root/.kaggle/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! cp ../input/apitoken/kaggle.json /root/.kaggle/kaggle.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the API key for the dataset\n\n#! cp ../input/api-token/kaggle.json /root/.kaggle/kaggle.json\n! mkdir -p /kaggle/tmp/bms_train\n! kaggle datasets init -p /kaggle/tmp/bms_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%bash\necho \"{\n  \\\"title\\\": \\\"CUSTOMDATABMS\\\",\n  \\\"id\\\": \\\"sudhirpol522/BMSDATA\\\",\n  \\\"licenses\\\": [\n    {\n      \\\"name\\\": \\\"CC0-1.0\\\"\n    }\n                ]\n}\" > /kaggle/tmp/bms_train/dataset-metadata.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! cp -rv ./ /kaggle/tmp/bms_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! kaggle datasets create -p /kaggle/tmp/bms_train/ -u --dir-mode tar","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Below two cells shows that, the dataset that we have generated is valid or not.","metadata":{}},{"cell_type":"code","source":"# convert in int encoded InChI to string\ndef inchi_int2char(InChI):\n    res = []\n    for i in InChI:\n        c = int_to_vocabulary.get(i)\n        if c not in ['<start>', '<end>', '<pad>']:\n            res.append(c)\n    return ''.join(res)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check train TFRecords\ndef decode_tfrecord(record_bytes):\n    features = tf.io.parse_single_example(record_bytes, {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'InChI': tf.io.FixedLenFeature([MAX_INCHI_LEN], tf.int64),\n    })\n\n    image = tf.io.decode_jpeg(features['image'])    \n    image = tf.reshape(image, [IMG_HEIGHT, IMG_WIDTH, 1])\n    image = tf.cast(image, tf.float32)  / 255.0\n    \n    InChI = features['InChI']\n    InChI = tf.reshape(InChI, [MAX_INCHI_LEN])\n    \n    return image, InChI\n\ndef show_tfrecords(file_path, rows=3, cols=2):\n    fig, ax = plt.subplots(rows, cols, figsize=(cols*7, rows*4))\n    tfrecord = tf.data.TFRecordDataset(file_path)\n    for idx, (image, InChI) in enumerate(tfrecord.map(decode_tfrecord).take(rows*cols)):\n        if idx == 0:\n            print(f'first InChI int: {InChI}')\n            print(f'first InChI char {inchi_int2char(InChI.numpy())}')\n        image = tf.cast(image * 255, tf.uint8)\n        image = tf.squeeze(image)\n        row, col = idx // cols, idx % cols\n        ax[row, col].imshow(image)\n\n    plt.show()\n\nprint('TRAIN BATCH')\nshow_tfrecords(f'./train/batch_0.tfrecords')\nprint('VAL BATCH')\nshow_tfrecords(f'./val/batch_0.tfrecords')","metadata":{},"execution_count":null,"outputs":[]}]}